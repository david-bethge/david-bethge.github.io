<!DOCTYPE HTML>
<html lang="en"><head>
  <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-LHYR833CDW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-LHYR833CDW');
</script>
  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>David Bethge</title>

  <meta name="author" content="David Bethge">
  <!-- mobile friendly view-->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">

	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üöÄ</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>David Bethge, Dr.</name>
              </p>
              <p>I am an Product Marketing Manager for high-resolution radar chips and software at <a href="https://www.nxp.com/"> NXP Semiconductors</a>. I love managing and executing projects or strategic initiatives in the field of technology. I am eager to connect with fascinating individuals and learn more about them.
              </p>
              <p>
                Previously, I worked at <a href="https://about.facebook.com/realitylabs/"> Meta/Facebook</a> as a researcher for machine learning and novel sensors for AR/VR input.
                I also worked as a Machine Learning Engineer and Innovation Manager for Emerging Technologies at <a href="https://www.porsche.com/">Porsche</a>.             
              </p>
              <p>
                I obtained a PhD (Dr. rer. nat.) in Computer Science at LMU Munich advised by <a href="https://uni.ubicomp.net/as/"> Albrecht Schmidt</a>.
                I have a masters and bachelors degree in Industrial Engineering and Management from KIT in Germany. In 2018/2019 I was a visiting researcher at <a href="https://www.cmu.edu/"> Carnegie Mellon University (CMU)</a> with <a href="https://www.ri.cmu.edu/ri-faculty/artur-w-dubrawski/"> Artur Dubrawski</a>.
              </p>
              <p style="text-align:center">
                <!--
                <a href="mailto:david.bethge1@gmail.com">Email</a> &nbsp/&nbsp
                -->
                <a href="https://www.linkedin.com/in/david-bethge/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=Ph0ocNoAAAAJ&hl=en&oi=ao">Google Scholar</a> 
                

                <!-- &nbsp/&nbsp -->
                <!-- <a href="https://github.com/jonbarron/">Github</a> -->
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/davidbethge.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/davidbethge.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                While I love working on cutting edge technologies, I am most intested in bringing them into products.             
              </p>
              <p>
                I have written over 15 research papers and have over 10 patents. Visit <a href="https://scholar.google.com/citations?user=Ph0ocNoAAAAJ&hl=en&oi=ao">Google Scholar</a> for a complete and up-to-date list.
                <!-- Representative papers are <span class="highlight">highlighted</span>. -->
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Publications</heading>
          </td>
        </tr>
      </tbody></table>
        

      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <tr onmouseout="malle_stop()" onmouseover="malle_start()"> -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <img src='images/happyrouting.png' width="160"></div>
                <img src='images/happyrouting.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="ttps://arxiv.org/html/2401.15695v1">
                <papertitle>HappyRouting: Learning Emotion-Aware Route Trajectories for Scalable In-The-Wild Navigation</papertitle>
              </a>
              <br>
              <strong>David Bethge</strong>,
              Daniel Bulanda, 
              Adam Kozlowski,
              <a href="https://thomaskosch.com/">Thomas Kosch</a>,
              <a href="https://uni.ubicomp.net/as/">Albrecht Schmidt</a>,
              <a href="http://grosse-puppendahl.com/">Tobias Grosse-Puppendahl</a>
              <br>
              <em>arxiv</em>, 2024
              <br>
              <!-- <a href="https://yifanjiang.net/MalleConv.html">project page</a> -->
              <!-- / -->
              <a href="https://arxiv.org/html/2401.15695v1">paper</a> 
              <p></p>
              <p>
                Novel navigation algorithm that finds the "happy" route. Using contextual information and machine learning to predict most likely happy route elements everywhere in the world.
              </p>
            </td>
          </tr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <tr onmouseout="malle_stop()" onmouseover="malle_start()"> -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <img src='images/phd_thesis.png' width="160"></div>
                <img src='images/phd_thesis.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://edoc.ub.uni-muenchen.de/32270/1/Bethge_David.pdf">
                <papertitle>Machine learning systems for human emotional states</papertitle>
              </a>
              <br>
              <strong>David Bethge</strong>
              <br>
              <em>Dissertation / PhD thesis</em>, 2023
              <br>
              <!-- <a href="https://yifanjiang.net/MalleConv.html">project page</a> -->
              <!-- / -->
              <a href="https://edoc.ub.uni-muenchen.de/32270/1/Bethge_David.pdf">PhD thesis</a> 
              <p></p>
            </td>
          </tr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <tr onmouseout="malle_stop()" onmouseover="malle_start()"> -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <img src='images/imwut.jpg' width="160"></div>
                <img src='images/imwut.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://thomaskosch.com/wp-content/papercite-data/pdf/bethge2023technical.pdf">
                <papertitle>Technical design space analysis for unobtrusive driver emotion assessment using multi-domain context</papertitle>
              </a>
              <br>
              <strong>David Bethge</strong>,
              Luis Falconeri Coelho,
              <a href="https://thomaskosch.com/">Thomas Kosch</a>,
              Satiyabooshan Murugaboopathy,
              <a href="https://code.berlin/de/team/ulrich-von-zadow/">Ulrich von Zadow</a>,
              <a href="https://uni.ubicomp.net/as/">Albrecht Schmidt</a>,
              <a href="http://grosse-puppendahl.com/">Tobias Grosse-Puppendahl</a>
              <br>
              <em>Ubicomp / IMWUT</em>, 2023
              <br>
              <!-- <a href="https://yifanjiang.net/MalleConv.html">project page</a> -->
              <!-- / -->
              <a href="https://thomaskosch.com/wp-content/papercite-data/pdf/bethge2023technical.pdf">paper</a> 
              <p></p>
              <p>
                Research explores non-intrusive prediction of driver emotions through contextual smartphone data, outperforming facial recognition by 7% in a study of 27 participants.
              </p>
            </td>
          </tr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <tr onmouseout="malle_stop()" onmouseover="malle_start()"> -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <img src='images/itber.jpg' width="160"></div>
                <img src='images/itber.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://thomaskosch.com/wp-content/papercite-data/pdf/bethge2023interpretable.pdf">
                <papertitle>Interpretable Time-Dependent Convolutional Emotion Recognition with Contextual Data Streams</papertitle>
              </a>
              <br>
              <strong>David Bethge</strong>,
              Constantin Patsch,
              <a href="https://philipph77.github.io/">Philipp Hallgarten</a>, 
              <a href="https://thomaskosch.com/">Thomas Kosch</a>
              <br>
              <em>CHI EA</em>, 2023
              <br>
              <!-- <a href="https://yifanjiang.net/MalleConv.html">project page</a> -->
              <!-- / -->
              <a href="https://thomaskosch.com/wp-content/papercite-data/pdf/bethge2023interpretable.pdf">paper</a> 
              <p></p>
              <p>
                Convolution-based neural network for emotion classification with interpretable time- and feature-aware model decisions, tested on a real-world driving dataset.
              </p>
            </td>
          </tr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <tr onmouseout="malle_stop()" onmouseover="malle_start()"> -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <img src='images/eusipco2023.jpg' width="160"></div>
                <img src='images/eusipco2023.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2306.06522.pdf">
                <papertitle>TS-MoCo: Time-Series Momentum Contrast for Self-Supervised Physiological Representation Learning</papertitle>
              </a>
              <br>
              <a href="https://philipph77.github.io/">Philipp Hallgarten</a>, 
              <strong>David Bethge</strong>,
              <a href="https://oozdenizci.github.io/">Ozan √ñzdenizci</a>,
              <a href="http://grosse-puppendahl.com/">Tobias Grosse-Puppendahl</a>,
              <a href="https://www.professoren.tum.de/kasneci-enkelejda">Enkelejda Kasneci</a>
              <br>
              <em>EUSIPCO</em>, 2023
              <br>
              <!-- <a href="https://yifanjiang.net/MalleConv.html">project page</a> -->
              <!-- / -->
              <a href="https://arxiv.org/pdf/2306.06522.pdf">paper</a> /
              <a href="https://github.com/philipph77/ts-moco">code</a>
              <p></p>
              <p>
                Self-supervised learning framework based on a transformer architecture for unlabeled physiological time-series, efficient for domain-agnostic systems in biomedical applications.
              </p>
            </td>
          </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <tr onmouseout="malle_stop()" onmouseover="malle_start()"> -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <img src='images/smc.jpg' width="160"></div>
                <img src='images/smc.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2207.08002.pdf">
                <papertitle>EEG2Vec: Learning Affective EEG Representations via Variational Autoencoders</papertitle>
              </a>
              <br>
              <strong>David Bethge</strong>,
              <a href="https://philipph77.github.io/">Philipp Hallgarten</a>, 
              <a href="http://grosse-puppendahl.com/">Tobias Grosse-Puppendahl</a>,
              <a href="https://mkari.de/">Mohamed Kari</a>,
							<a href="http://www.lewischuang.com/">Lewis L. Chuang</a>,
              <a href="https://oozdenizci.github.io/">Ozan √ñzdenizci</a>,
              <a href="https://uni.ubicomp.net/as/">Albrecht Schmidt</a>
              <br>
              <em>SMC</em>, 2022
              <br>
              <!-- <a href="https://yifanjiang.net/MalleConv.html">project page</a> -->
              <!-- / -->
              <a href="https://arxiv.org/pdf/2207.08002.pdf">paper</a>
              <p></p>
              <p>
                End-to-end representation learning framework for modeling user-specific affective representation from raw EEG.
              </p>
            </td>
          </tr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <tr onmouseout="malle_stop()" onmouseover="malle_start()"> -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <img src='images/icassp.jpg' width="160"></div>
                <img src='images/icassp.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2201.11613">
                <papertitle>Domain-Invariant Representation Learning from EEG with Private Encoders</papertitle>
              </a>
              <br>
              <strong>David Bethge</strong>,
              <a href="https://philipph77.github.io/">Philipp Hallgarten</a>, 
              <a href="http://grosse-puppendahl.com/">Tobias Grosse-Puppendahl</a>,
              <a href="https://mkari.de/">Mohamed Kari</a>,
              Ralf Mikut,
              <a href="https://uni.ubicomp.net/as/">Albrecht Schmidt</a>,
              <a href="https://oozdenizci.github.io/">Ozan √ñzdenizci</a>
              <br>
              <em>ICASSP</em>, 2022
              <br>
              <!-- <a href="https://yifanjiang.net/MalleConv.html">project page</a> -->
              <!-- / -->
              <a href="https://arxiv.org/abs/2201.11613">paper</a>
              <p></p>
              <p>
                Multi-source deep learning network that is able to learn domain-invariant latent representation from multiple data-specific private encoders.
              </p>
            </td>
          </tr>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <!-- <tr onmouseout="malle_stop()" onmouseover="malle_start()"> -->
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='malle_image'>
                    <img src='images/embc.jpg' width="160"></div>
                  <img src='images/embc.jpg' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2204.07777.pdf">
                  <papertitle>Exploiting Multiple EEG Data Domains with Adversarial Learning</papertitle>
                </a>
                <br>
                <strong>David Bethge</strong>,
                <a href="https://philipph77.github.io/">Philipp Hallgarten</a>, 
                <a href="https://oozdenizci.github.io/">Ozan √ñzdenizci</a>,
                Ralf Mikut,
                <a href="https://uni.ubicomp.net/as/">Albrecht Schmidt</a>,
                <a href="http://grosse-puppendahl.com/">Tobias Grosse-Puppendahl</a>
                <br>
                <em>EMBC</em>, 2022
                <br>
                <a href="https://arxiv.org/pdf/2204.07777.pdf">paper</a> /
                <a href="https://github.com/philipph77/ACSE-Framework">code</a>
                <p></p>
                <p>
                  Enabling multi-source learning for EEG-based brain-computer interfaces via adversarial representation learning.
                </p>
              </td>
            </tr>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <tr onmouseout="malle_stop()" onmouseover="malle_start()"> -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <img src='images/teaserfig_vemotion.png' width="160"></div>
                <img src='images/teaserfig_vemotion.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://thomaskosch.com/wp-content/papercite-data/pdf/bethge2021vemotion.pdf">
                <papertitle>VEmotion: Using Driving Context for Indirect Emotion Prediction in Real-Time</papertitle>
              </a>
              <br>
              <strong>David Bethge</strong>,
              <a href="https://thomaskosch.com/">Thomas Kosch</a>,
              <a href="http://grosse-puppendahl.com/">Tobias Grosse-Puppendahl</a>,
							<a href="http://www.lewischuang.com/">Lewis L. Chuang</a>, <br>
              <a href="https://mkari.de/">Mohamed Kari</a>,
              Alexander Jagaciak,
              <a href="https://uni.ubicomp.net/as/">Albrecht Schmidt</a>
              <br>
              <em>UIST</em>, 2021
              <br>
              <!-- <a href="https://yifanjiang.net/MalleConv.html">project page</a> -->
              <!-- / -->
              <a href="https://thomaskosch.com/wp-content/papercite-data/pdf/bethge2021vemotion.pdf">paper</a> /
              <a href="https://github.com/davebeght/VEmotion">code</a>
              <p></p>
              <p>
              Remote sensing system that analyzes traffic dynamics, environmental factors, in-vehicle context, and road characteristics to implicitly classify driver emotions.
              </p>
            </td>
          </tr>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <!-- <tr onmouseout="malle_stop()" onmouseover="malle_start()"> -->
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='malle_image'>
                    <img src='images/soundsride.png' width="160"></div>
                  <img src='images/soundsride.png' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://3dvar.com/Kari2021SoundsRide.pdf">
                  <papertitle>SoundsRide: Affordance-Synchronized Music Mixing for In-Car Audio Augmented Reality</papertitle>
                </a>
                <br>
                <a href="https://mkari.de/">Mohamed Kari</a>,
                <a href="http://grosse-puppendahl.com/">Tobias Grosse-Puppendahl</a>,
                Alexander Jagaciak,
                <strong>David Bethge</strong>,
                Reinhard Sch√ºtte,
                <a href="https://www.christianholz.net/">Christian Holz</a>
                <br>
                <em>UIST</em>, 2021 (Best Paper Award)
                <br>
                <!-- <a href="https://yifanjiang.net/MalleConv.html">project page</a> -->
                <!-- / -->
                <a href="https://3dvar.com/Kari2021SoundsRide.pdf">paper</a>
                <p></p>
                <p>
                  In-car audio augmented reality system that mixes music in real-time synchronized with sound affordances along the ride.
                </p>
              </td>
            </tr>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <!-- <tr onmouseout="malle_stop()" onmouseover="malle_start()"> -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/ismar.png' width="160"></div>
                    <img src='images/ismar.png' width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://mkari.de/pubs/ismar2021-transformr.pdf">
                    <papertitle>TransforMR: Pose-Aware Object Substitution for Composing Alternate Mixed Realities</papertitle>
                  </a>
                  <br>
                  <a href="https://mkari.de/">Mohamed Kari</a>,
                  <a href="http://grosse-puppendahl.com/">Tobias Grosse-Puppendahl</a>,
                  Luis Falconeri Coelho,
                  Andreas Fender,
                  <strong>David Bethge</strong>,
                  Reinhard Sch√ºtte,
                  <a href="https://www.christianholz.net/">Christian Holz</a>
                  <br>
                  <em>ISMAR</em>, 2021
                  <br>
                  <!-- <a href="https://yifanjiang.net/MalleConv.html">project page</a> -->
                  <!-- / -->
                  <a href="https://mkari.de/pubs/ismar2021-transformr.pdf">paper</a>
                  <p></p>
                  <p>
                    Mixed reality system for mobile devices that performs 3D-pose-aware object substitution to create meaningful mixed reality scenes.
                  </p>
                </td>
              </tr>



            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <!-- <tr onmouseout="malle_stop()" onmouseover="malle_start()"> -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/hminference.png' width="160"></div>
                    <img src='images/hminference.png' width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://dl.acm.org/doi/abs/10.1145/3409118.3475145">
                    <papertitle>HMInference: Inferring Multimodal HMI Interactions in Automotive Screens</papertitle>
                  </a>
                  <br>
                  <a>Jannik Wolf</a>,
                  <a>Marco Wiedner</a>
                  <a href="https://mkari.de/">Mohamed Kari</a>,
                  <strong>David Bethge</strong>
                  <br>
                  <em>AutoUI</em>, 2021
                  <br>
                  <!-- <a href="https://yifanjiang.net/MalleConv.html">project page</a> -->
                  <!-- / -->
                  <a href="https://dl.acm.org/doi/abs/10.1145/3409118.3475145">paper</a>
                  <p></p>
                  <p>
                    System to predict interactions in the car HMI based on contextual CAN-BUS data.
                  </p>
                </td>
              </tr>


              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                <!-- <tr onmouseout="malle_stop()" onmouseover="malle_start()"> -->
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='malle_image'>
                        <img src='images/icdm.jpg' width="160"></div>
                      <img src='images/icdm.jpg' width="160">
                    </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://ieeexplore.ieee.org/document/8955539">
                      <papertitle>Prognostication of Neurological Recovery by Analyzing Structural Breaks in EEG Data</papertitle>
                    </a>
                    <br>
                    <strong>David Bethge</strong>,
                    <a>Jieshi Chen</a>,
                    <a>Oliver Grothe</a>,
                    <a>Jonathan Elmer</a>,
                    <a>Artur Dubrawski</a>
                    <br>
                    <em>ICDM Workshop</em>, 2019
                    <br>
                    <!-- <a href="https://yifanjiang.net/MalleConv.html">project page</a> -->
                    <!-- / -->
                    <a href="https://ieeexplore.ieee.org/document/8955539">paper</a>
                    <p></p>
                    <p>
                      Unsupervised, multivariate yet interpretable structural break testing for prognostication of neurological recovery after cardiac arrest.
                    </p>
                  </td>
                </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Patents</heading>
                <p>
                  P1: DE/102022120257A1, ‚ÄúVerfahren, Wearable, Vorrichtung und Fahrzeug zur Emotionalisierung eines Fahrererlebnisses im Fahrzeug‚Äù (2024)
                </p>
                <p>
                  P2: US/20240027215A1, ‚ÄúComputer-implemented method for determining a navigation route‚Äù (2024)
                </p>
                <p>
                  P3: DE/102022113585A1, ‚ÄúComputerimplementiertes Verfahren zum Verbessern einer Performance einer Cockpit-Nutzerschnittstelle‚Äú(2023)
                </p>
                <p>
                  P4: DE/102022110349A1, ‚ÄúVerfahren und Vorrichtung zur √úberwachung von Objekten‚Äù (2023)
                </p>
                <p>
                  P5: DE/102022106797B3, ‚ÄúVerfahren zur automatischen Einstellung zumindest eines R√ºckspiegels eines Kraftfahrzeugs‚Äù (2023)
                </p>
                <p>
                  P6: DE/102022106812B4, ‚ÄúComputerimplementiertes Verfahren zur Ermittlung eines Emotionszustandes einer Person in einem Kraftfahrzeug‚Äù (2024)
                </p>
                <p>
                  P7: DE/102022104325A1, ‚ÄúVerfahren, System und Computerprogrammprodukt zur Erstellung einer Datenstruktur f√ºr auf k√ºnstlicher Intelligenz basierende Projekte‚Äù (2023)
                </p>
                <p>
                  P8: DE102022104322A1, ‚ÄúVorrichtung und Verfahren zur Fahrerassistenz f√ºr ein Kraftfahrzeug, die Vorrichtung umfassendes Kraftfahrzeug‚Äù (2023)
                </p>
                <p>
                  P9: DE/102021130939B3, ‚ÄúVorrichtungen und Verfahren zur gemeinsamen Routenf√ºhrung‚Äù (2023)
                </p>
                <p>
                  P10: US/20230147024A1, ‚ÄúMethod and system for robust identification of a vehicle occupant‚Äù (2023)
                </p>
                <p>
                  P11: DE/102021129108A1, ‚ÄúVerfahren, System und Computerprogrammprodukt zum Ausgeben von Ausgabewerten zur Analyse und/oder Bewertung und/oder Prozesssteuerung einer Entit√§t‚Äù (2023)
                </p>
                <p>
                  P12: US/20230146013A1, ‚ÄúMethod for producing a model for automated prediction of interactions of a user with a user interface of a motor vehicle‚Äù (2023)
                </p>
                <p>
                  P13: DE/102021129094A1, ‚ÄúVerfahren zum Ermitteln von Ersatzteilbedarf‚Äù (2023)
                </p>
                <p>
                  P14: US/20230020786A1 ‚ÄúSystem for a motor vehicle and method for assessing the emotions of a driver of a motor vehicle‚Äù (2023)
                </p>
                <p>
                  P15: DE/102021116641A1, ‚ÄúVerfahren, System und Computerprogramm zur interaktiven Auswahl und Wiedergabe von in Echtzeit erzeugten Audio- und/oder Videosequenzen in einem Kraftfahrzeug‚Äù (2022)
                </p>
                <p>
                  P16: DE/102021110268A1, ‚ÄúVerfahren und System zur szenensynchronen Auswahl und Wiedergabe von Audiosequenzen f√ºr ein Kraftfahrzeug‚Äù (2022)
                </p>
              </td>
            </tr>

        





        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Talks / Lectures / Podcasts</heading>
              <p>
                I am regularly giving talks and keynotes about technology in general. Feel free to reach out.              
              </p>             
            </td>
          </tr>
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <tr onmouseout="malle_stop()" onmouseover="malle_start()"> -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                 <!--  <img src='images/doktoranden-podcast-logo.png' width="160"></div>
                <img src='images/doktoranden-podcast-logo.png' width="160"> -->
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle> Visiting Lecturer for a Machine Learning introductory course at Baden-Wuerrtemberg Cooperative State University</papertitle>
              <br>
              <p>
                In 2018 along with a fellow student I lectured a machine learning introductory class for computer science students at Baden-Wuerrtemberg Cooperative State University (DHBW Karlsruhe). 
                <a href="https://github.com/ferreirafabio/Intro_to_ML_DHBW">[lecture material]</a>
              </p>
            </td>
          </tr>
        </tbody>
        </table>




<!--
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <tr onmouseout="malle_stop()" onmouseover="malle_start()"> 
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <img src='images/hilti_view.jpeg' width="160"></div>
                <img src='images/hilti_view.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a>
                <papertitle>Invited talks</papertitle>
              </a>
              <br>
              <p>
                At Hilti, I have initiated, produced, and together with my fellow head of the PhD network Mohamed Kari, co-hosted the Porsche PhD Podcast where we interviewed Porsche PhD students company-internally on their research and Porsche executives on the role of PhD research for innovation.</p>
            </td>
          </tr>
        </tbody>
        </table>

 -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <tr onmouseout="malle_stop()" onmouseover="malle_start()"> -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <img src='images/doktoranden-podcast-logo.png' width="160"></div>
                <img src='images/doktoranden-podcast-logo.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle> Porsche PhD Podcast</papertitle>
              <br>
              <p>
                At Porsche, I have initiated, produced, and together with my fellow head of the PhD network Mohamed Kari, co-hosted the Porsche PhD Podcast where we interviewed Porsche PhD students company-internally on their research and Porsche executives on the role of PhD research for innovation.</p>
            </td>
          </tr>
        </tbody>
        </table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Personal Life</heading>
            <p>
              I come from a family of engineers and entrepeneurs. I grew up in Germany and Austria traveling a lot while I was a kid. I was raised with a strong emphasis on education, lifelong learning, kindness, and the strong desire to think in-depth about the world around us.
            </p>
            <p>  
              In my free time, I find joy climbing in the alpes with my friends and surfing the waves along Europe's coastlines. 
            </p>
          </td>
        </tr>
        </tbody>
      </table>






        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                This website was forked from Jon Barron's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. Thanks!
                <br>
                <!-- Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page. -->
              </p>
            </td>
          </tr>
        </tbody></table>
       </td>
    </tr>
  </table>
  <!--  <div class='impressum'><h1>Impressum</h1><p>Angaben gem√§√ü ¬ß 5 TMG</p><p>David Bethge<br> 
    Baiersdorferstr 12<br> 
    22529 Hamburg <br> 
    E-Mail: <a href='mailto:david.bethge@ifi.lmu.de'>david.bethge@ifi.lmu.de</a></br></p><p> <br>
    <p><strong>Haftungsausschluss: </strong><br><br><strong>Haftung f√ºr Inhalte</strong><br><br>
    Die Inhalte unserer Seiten wurden mit gr√∂√üter Sorgfalt erstellt. F√ºr die Richtigkeit, Vollst√§ndigkeit und Aktualit√§t der Inhalte k√∂nnen wir jedoch keine Gew√§hr √ºbernehmen. Als Diensteanbieter sind wir gem√§√ü ¬ß 7 Abs.1 TMG f√ºr eigene Inhalte auf diesen Seiten nach den allgemeinen Gesetzen verantwortlich. Nach ¬ß¬ß 8 bis 10 TMG sind wir als Diensteanbieter jedoch nicht verpflichtet, √ºbermittelte oder gespeicherte fremde Informationen zu √ºberwachen oder nach Umst√§nden zu forschen, die auf eine rechtswidrige T√§tigkeit hinweisen. Verpflichtungen zur Entfernung oder Sperrung der Nutzung von Informationen nach den allgemeinen Gesetzen bleiben hiervon unber√ºhrt. Eine diesbez√ºgliche Haftung ist jedoch erst ab dem Zeitpunkt der Kenntnis einer konkreten Rechtsverletzung m√∂glich. Bei Bekanntwerden von entsprechenden Rechtsverletzungen werden wir diese Inhalte umgehend entfernen.<br><br><strong>Haftung f√ºr Links</strong><br><br>
    Unser Angebot enth√§lt Links zu externen Webseiten Dritter, auf deren Inhalte wir keinen Einfluss haben. Deshalb k√∂nnen wir f√ºr diese fremden Inhalte auch keine Gew√§hr √ºbernehmen. F√ºr die Inhalte der verlinkten Seiten ist stets der jeweilige Anbieter oder Betreiber der Seiten verantwortlich. Die verlinkten Seiten wurden zum Zeitpunkt der Verlinkung auf m√∂gliche Rechtsverst√∂√üe √ºberpr√ºft. Rechtswidrige Inhalte waren zum Zeitpunkt der Verlinkung nicht erkennbar. Eine permanente inhaltliche Kontrolle der verlinkten Seiten ist jedoch ohne konkrete Anhaltspunkte einer Rechtsverletzung nicht zumutbar. Bei Bekanntwerden von Rechtsverletzungen werden wir derartige Links umgehend entfernen.<br><br><strong>Urheberrecht</strong><br><br>
    Die durch die Seitenbetreiber erstellten Inhalte und Werke auf diesen Seiten unterliegen dem deutschen Urheberrecht. Die Vervielf√§ltigung, Bearbeitung, Verbreitung und jede Art der Verwertung au√üerhalb der Grenzen des Urheberrechtes bed√ºrfen der schriftlichen Zustimmung des jeweiligen Autors bzw. Erstellers. Downloads und Kopien dieser Seite sind nur f√ºr den privaten, nicht kommerziellen Gebrauch gestattet. Soweit die Inhalte auf dieser Seite nicht vom Betreiber erstellt wurden, werden die Urheberrechte Dritter beachtet. Insbesondere werden Inhalte Dritter als solche gekennzeichnet. Sollten Sie trotzdem auf eine Urheberrechtsverletzung aufmerksam werden, bitten wir um einen entsprechenden Hinweis. Bei Bekanntwerden von Rechtsverletzungen werden wir derartige Inhalte umgehend entfernen.<br><br><strong>Datenschutz</strong><br><br>
    Die Nutzung unserer Webseite ist in der Regel ohne Angabe personenbezogener Daten m√∂glich. Soweit auf unseren Seiten personenbezogene Daten (beispielsweise Name, Anschrift oder eMail-Adressen) erhoben werden, erfolgt dies, soweit m√∂glich, stets auf freiwilliger Basis. Diese Daten werden ohne Ihre ausdr√ºckliche Zustimmung nicht an Dritte weitergegeben. <br>
    Wir weisen darauf hin, dass die Daten√ºbertragung im Internet (z.B. bei der Kommunikation per E-Mail) Sicherheitsl√ºcken aufweisen kann. Ein l√ºckenloser Schutz der Daten vor dem Zugriff durch Dritte ist nicht m√∂glich. <br>
    Der Nutzung von im Rahmen der Impressumspflicht ver√∂ffentlichten Kontaktdaten durch Dritte zur √úbersendung von nicht ausdr√ºcklich angeforderter Werbung und Informationsmaterialien wird hiermit ausdr√ºcklich widersprochen. Die Betreiber der Seiten behalten sich ausdr√ºcklich rechtliche Schritte im Falle der unverlangten Zusendung von Werbeinformationen, etwa durch Spam-Mails, vor.<br>
    <br><br><strong>Google Analytics</strong><br><br>
    Diese Website benutzt Google Analytics, einen Webanalysedienst der Google Inc. (''Google''). Google Analytics verwendet sog. ''Cookies'', Textdateien, die auf Ihrem Computer gespeichert werden und die eine Analyse der Benutzung der Website durch Sie erm√∂glicht. Die durch den Cookie erzeugten Informationen √ºber Ihre Benutzung dieser Website (einschlie√ülich Ihrer IP-Adresse) wird an einen Server von Google in den USA √ºbertragen und dort gespeichert. Google wird diese Informationen benutzen, um Ihre Nutzung der Website auszuwerten, um Reports √ºber die Websiteaktivit√§ten f√ºr die Websitebetreiber zusammenzustellen und um weitere mit der Websitenutzung und der Internetnutzung verbundene Dienstleistungen zu erbringen. Auch wird Google diese Informationen gegebenenfalls an Dritte √ºbertragen, sofern dies gesetzlich vorgeschrieben oder soweit Dritte diese Daten im Auftrag von Google verarbeiten. Google wird in keinem Fall Ihre IP-Adresse mit anderen Daten der Google in Verbindung bringen. Sie k√∂nnen die Installation der Cookies durch eine entsprechende Einstellung Ihrer Browser Software verhindern; wir weisen Sie jedoch darauf hin, dass Sie in diesem Fall gegebenenfalls nicht s√§mtliche Funktionen dieser Website voll umf√§nglich nutzen k√∂nnen. Durch die Nutzung dieser Website erkl√§ren Sie sich mit der Bearbeitung der √ºber Sie erhobenen Daten durch Google in der zuvor beschriebenen Art und Weise und zu dem zuvor benannten Zweck einverstanden.<br><br><strong>Google AdSense</strong><br><br>
    Diese Website benutzt Google Adsense, einen Webanzeigendienst der Google Inc., USA (''Google''). Google Adsense verwendet sog. ''Cookies'' (Textdateien), die auf Ihrem Computer gespeichert werden und die eine Analyse der Benutzung der Website durch Sie erm√∂glicht. Google Adsense verwendet auch sog. ''Web Beacons'' (kleine unsichtbare Grafiken) zur Sammlung von Informationen. Durch die Verwendung des Web Beacons k√∂nnen einfache Aktionen wie der Besucherverkehr auf der Webseite aufgezeichnet und gesammelt werden. Die durch den Cookie und/oder Web Beacon erzeugten Informationen √ºber Ihre Benutzung dieser Website (einschlie√ülich Ihrer IP-Adresse) werden an einen Server von Google in den USA √ºbertragen und dort gespeichert. Google wird diese Informationen benutzen, um Ihre Nutzung der Website im Hinblick auf die Anzeigen auszuwerten, um Reports √ºber die Websiteaktivit√§ten und Anzeigen f√ºr die Websitebetreiber zusammenzustellen und um weitere mit der Websitenutzung und der Internetnutzung verbundene Dienstleistungen zu erbringen. Auch wird Google diese Informationen gegebenenfalls an Dritte √ºbertragen, sofern dies gesetzlich vorgeschrieben oder soweit Dritte diese Daten im Auftrag von Google verarbeiten. Google wird in keinem Fall Ihre IP-Adresse mit anderen Daten der Google in Verbindung bringen. Das Speichern von Cookies auf Ihrer Festplatte und die Anzeige von Web Beacons k√∂nnen Sie verhindern, indem Sie in Ihren Browser-Einstellungen ''keine Cookies akzeptieren'' w√§hlen (Im MS Internet-Explorer unter ''Extras > Internetoptionen > Datenschutz > Einstellung''; im Firefox unter ''Extras > Einstellungen > Datenschutz > Cookies''); wir weisen Sie jedoch darauf hin, dass Sie in diesem Fall gegebenenfalls nicht s√§mtliche Funktionen dieser Website voll umf√§nglich nutzen k√∂nnen. Durch die Nutzung dieser Website erkl√§ren Sie sich mit der Bearbeitung der √ºber Sie erhobenen Daten durch Google in der zuvor beschriebenen Art und Weise und zu dem zuvor benannten Zweck einverstanden.</p><br> 
    Website Impressum erstellt durch <a href="https://www.impressum-generator.de">impressum-generator.de</a> von der <a href="https://www.kanzlei-hasselbach.de/" rel="nofollow">Kanzlei Hasselbach</a>
     </div>
     -->

     
</body>

</html>
