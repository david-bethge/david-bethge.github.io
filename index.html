<!DOCTYPE HTML>
<html lang="en"><head>
  <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-LHYR833CDW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-LHYR833CDW');
</script>
  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>David Bethge</title>

  <meta name="author" content="David Bethge">
  <!-- mobile friendly view-->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">

	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🚀</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>David Bethge, Dr.</name>
              </p>
              <p>I am an Product Marketing Manager for high-resolution radar chips and software at <a href="https://www.nxp.com/"> NXP Semiconductors</a>. I love managing and executing projects or strategic initiatives in the field of technology. I am eager to connect with fascinating individuals and learn more about them.
              </p>
              <p>
                Previously, I worked at <a href="https://about.facebook.com/realitylabs/"> Meta/Facebook</a> as a researcher for machine learning and novel sensors for AR/VR input.
                I also worked as a Machine Learning Engineer and Innovation Manager for Emerging Technologies at <a href="https://www.porsche.com/">Porsche</a>.             
              </p>
              <p>
                I obtained a PhD (Dr. rer. nat.) in Computer Science at LMU Munich advised by <a href="https://uni.ubicomp.net/as/"> Albrecht Schmidt</a>.
                I have a masters and bachelors degree in Industrial Engineering and Management from KIT in Germany. In 2018/2019 I was a visiting researcher at <a href="https://www.cmu.edu/"> Carnegie Mellon University (CMU)</a> with <a href="https://www.ri.cmu.edu/ri-faculty/artur-w-dubrawski/"> Artur Dubrawski</a>.
              </p>
              <p style="text-align:center">
                <!--
                <a href="mailto:david.bethge1@gmail.com">Email</a> &nbsp/&nbsp
                -->
                <a href="https://www.linkedin.com/in/david-bethge/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=Ph0ocNoAAAAJ&hl=en&oi=ao">Google Scholar</a> 
                

                <!-- &nbsp/&nbsp -->
                <!-- <a href="https://github.com/jonbarron/">Github</a> -->
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/davidbethge.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/davidbethge.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                While I love working on cutting edge technologies, I am most intested in bringing them into products.             
              </p>
              <p>
                I have written over 15 research papers and have over 10 patents. Visit <a href="https://scholar.google.com/citations?user=Ph0ocNoAAAAJ&hl=en&oi=ao">Google Scholar</a> for a complete and up-to-date list.
                <!-- Representative papers are <span class="highlight">highlighted</span>. -->
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Publications</heading>
          </td>
        </tr>
      </tbody></table>
        

      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <tr onmouseout="malle_stop()" onmouseover="malle_start()"> -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <img src='images/happyrouting.png' width="160"></div>
                <img src='images/happyrouting.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="ttps://arxiv.org/html/2401.15695v1">
                <papertitle>HappyRouting: Learning Emotion-Aware Route Trajectories for Scalable In-The-Wild Navigation</papertitle>
              </a>
              <br>
              <strong>David Bethge</strong>,
              Daniel Bulanda, 
              Adam Kozlowski,
              <a href="https://thomaskosch.com/">Thomas Kosch</a>,
              <a href="https://uni.ubicomp.net/as/">Albrecht Schmidt</a>,
              <a href="http://grosse-puppendahl.com/">Tobias Grosse-Puppendahl</a>
              <br>
              <em>arxiv</em>, 2024
              <br>
              <!-- <a href="https://yifanjiang.net/MalleConv.html">project page</a> -->
              <!-- / -->
              <a href="https://arxiv.org/html/2401.15695v1">paper</a> 
              <p></p>
              <p>
                Novel navigation algorithm that finds the "happy" route. Using contextual information and machine learning to predict most likely happy route elements everywhere in the world.
              </p>
            </td>
          </tr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <tr onmouseout="malle_stop()" onmouseover="malle_start()"> -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <img src='images/phd_thesis.png' width="160"></div>
                <img src='images/phd_thesis.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://edoc.ub.uni-muenchen.de/32270/1/Bethge_David.pdf">
                <papertitle>Machine learning systems for human emotional states</papertitle>
              </a>
              <br>
              <strong>David Bethge</strong>
              <br>
              <em>Dissertation / PhD thesis</em>, 2023
              <br>
              <!-- <a href="https://yifanjiang.net/MalleConv.html">project page</a> -->
              <!-- / -->
              <a href="https://edoc.ub.uni-muenchen.de/32270/1/Bethge_David.pdf">PhD thesis</a> 
              <p></p>
            </td>
          </tr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <tr onmouseout="malle_stop()" onmouseover="malle_start()"> -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <img src='images/imwut.jpg' width="160"></div>
                <img src='images/imwut.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://thomaskosch.com/wp-content/papercite-data/pdf/bethge2023technical.pdf">
                <papertitle>Technical design space analysis for unobtrusive driver emotion assessment using multi-domain context</papertitle>
              </a>
              <br>
              <strong>David Bethge</strong>,
              Luis Falconeri Coelho,
              <a href="https://thomaskosch.com/">Thomas Kosch</a>,
              Satiyabooshan Murugaboopathy,
              <a href="https://code.berlin/de/team/ulrich-von-zadow/">Ulrich von Zadow</a>,
              <a href="https://uni.ubicomp.net/as/">Albrecht Schmidt</a>,
              <a href="http://grosse-puppendahl.com/">Tobias Grosse-Puppendahl</a>
              <br>
              <em>Ubicomp / IMWUT</em>, 2023
              <br>
              <!-- <a href="https://yifanjiang.net/MalleConv.html">project page</a> -->
              <!-- / -->
              <a href="https://thomaskosch.com/wp-content/papercite-data/pdf/bethge2023technical.pdf">paper</a> 
              <p></p>
              <p>
                Research explores non-intrusive prediction of driver emotions through contextual smartphone data, outperforming facial recognition by 7% in a study of 27 participants.
              </p>
            </td>
          </tr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <tr onmouseout="malle_stop()" onmouseover="malle_start()"> -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <img src='images/itber.jpg' width="160"></div>
                <img src='images/itber.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://thomaskosch.com/wp-content/papercite-data/pdf/bethge2023interpretable.pdf">
                <papertitle>Interpretable Time-Dependent Convolutional Emotion Recognition with Contextual Data Streams</papertitle>
              </a>
              <br>
              <strong>David Bethge</strong>,
              Constantin Patsch,
              <a href="https://philipph77.github.io/">Philipp Hallgarten</a>, 
              <a href="https://thomaskosch.com/">Thomas Kosch</a>
              <br>
              <em>CHI EA</em>, 2023
              <br>
              <!-- <a href="https://yifanjiang.net/MalleConv.html">project page</a> -->
              <!-- / -->
              <a href="https://thomaskosch.com/wp-content/papercite-data/pdf/bethge2023interpretable.pdf">paper</a> 
              <p></p>
              <p>
                Convolution-based neural network for emotion classification with interpretable time- and feature-aware model decisions, tested on a real-world driving dataset.
              </p>
            </td>
          </tr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <tr onmouseout="malle_stop()" onmouseover="malle_start()"> -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <img src='images/eusipco2023.jpg' width="160"></div>
                <img src='images/eusipco2023.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2306.06522.pdf">
                <papertitle>TS-MoCo: Time-Series Momentum Contrast for Self-Supervised Physiological Representation Learning</papertitle>
              </a>
              <br>
              <a href="https://philipph77.github.io/">Philipp Hallgarten</a>, 
              <strong>David Bethge</strong>,
              <a href="https://oozdenizci.github.io/">Ozan Özdenizci</a>,
              <a href="http://grosse-puppendahl.com/">Tobias Grosse-Puppendahl</a>,
              <a href="https://www.professoren.tum.de/kasneci-enkelejda">Enkelejda Kasneci</a>
              <br>
              <em>EUSIPCO</em>, 2023
              <br>
              <!-- <a href="https://yifanjiang.net/MalleConv.html">project page</a> -->
              <!-- / -->
              <a href="https://arxiv.org/pdf/2306.06522.pdf">paper</a> /
              <a href="https://github.com/philipph77/ts-moco">code</a>
              <p></p>
              <p>
                Self-supervised learning framework based on a transformer architecture for unlabeled physiological time-series, efficient for domain-agnostic systems in biomedical applications.
              </p>
            </td>
          </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <tr onmouseout="malle_stop()" onmouseover="malle_start()"> -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <img src='images/smc.jpg' width="160"></div>
                <img src='images/smc.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2207.08002.pdf">
                <papertitle>EEG2Vec: Learning Affective EEG Representations via Variational Autoencoders</papertitle>
              </a>
              <br>
              <strong>David Bethge</strong>,
              <a href="https://philipph77.github.io/">Philipp Hallgarten</a>, 
              <a href="http://grosse-puppendahl.com/">Tobias Grosse-Puppendahl</a>,
              <a href="https://mkari.de/">Mohamed Kari</a>,
							<a href="http://www.lewischuang.com/">Lewis L. Chuang</a>,
              <a href="https://oozdenizci.github.io/">Ozan Özdenizci</a>,
              <a href="https://uni.ubicomp.net/as/">Albrecht Schmidt</a>
              <br>
              <em>SMC</em>, 2022
              <br>
              <!-- <a href="https://yifanjiang.net/MalleConv.html">project page</a> -->
              <!-- / -->
              <a href="https://arxiv.org/pdf/2207.08002.pdf">paper</a>
              <p></p>
              <p>
                End-to-end representation learning framework for modeling user-specific affective representation from raw EEG.
              </p>
            </td>
          </tr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <tr onmouseout="malle_stop()" onmouseover="malle_start()"> -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <img src='images/icassp.jpg' width="160"></div>
                <img src='images/icassp.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2201.11613">
                <papertitle>Domain-Invariant Representation Learning from EEG with Private Encoders</papertitle>
              </a>
              <br>
              <strong>David Bethge</strong>,
              <a href="https://philipph77.github.io/">Philipp Hallgarten</a>, 
              <a href="http://grosse-puppendahl.com/">Tobias Grosse-Puppendahl</a>,
              <a href="https://mkari.de/">Mohamed Kari</a>,
              Ralf Mikut,
              <a href="https://uni.ubicomp.net/as/">Albrecht Schmidt</a>,
              <a href="https://oozdenizci.github.io/">Ozan Özdenizci</a>
              <br>
              <em>ICASSP</em>, 2022
              <br>
              <!-- <a href="https://yifanjiang.net/MalleConv.html">project page</a> -->
              <!-- / -->
              <a href="https://arxiv.org/abs/2201.11613">paper</a>
              <p></p>
              <p>
                Multi-source deep learning network that is able to learn domain-invariant latent representation from multiple data-specific private encoders.
              </p>
            </td>
          </tr>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <!-- <tr onmouseout="malle_stop()" onmouseover="malle_start()"> -->
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='malle_image'>
                    <img src='images/embc.jpg' width="160"></div>
                  <img src='images/embc.jpg' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2204.07777.pdf">
                  <papertitle>Exploiting Multiple EEG Data Domains with Adversarial Learning</papertitle>
                </a>
                <br>
                <strong>David Bethge</strong>,
                <a href="https://philipph77.github.io/">Philipp Hallgarten</a>, 
                <a href="https://oozdenizci.github.io/">Ozan Özdenizci</a>,
                Ralf Mikut,
                <a href="https://uni.ubicomp.net/as/">Albrecht Schmidt</a>,
                <a href="http://grosse-puppendahl.com/">Tobias Grosse-Puppendahl</a>
                <br>
                <em>EMBC</em>, 2022
                <br>
                <a href="https://arxiv.org/pdf/2204.07777.pdf">paper</a> /
                <a href="https://github.com/philipph77/ACSE-Framework">code</a>
                <p></p>
                <p>
                  Enabling multi-source learning for EEG-based brain-computer interfaces via adversarial representation learning.
                </p>
              </td>
            </tr>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <tr onmouseout="malle_stop()" onmouseover="malle_start()"> -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <img src='images/teaserfig_vemotion.png' width="160"></div>
                <img src='images/teaserfig_vemotion.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://thomaskosch.com/wp-content/papercite-data/pdf/bethge2021vemotion.pdf">
                <papertitle>VEmotion: Using Driving Context for Indirect Emotion Prediction in Real-Time</papertitle>
              </a>
              <br>
              <strong>David Bethge</strong>,
              <a href="https://thomaskosch.com/">Thomas Kosch</a>,
              <a href="http://grosse-puppendahl.com/">Tobias Grosse-Puppendahl</a>,
							<a href="http://www.lewischuang.com/">Lewis L. Chuang</a>, <br>
              <a href="https://mkari.de/">Mohamed Kari</a>,
              Alexander Jagaciak,
              <a href="https://uni.ubicomp.net/as/">Albrecht Schmidt</a>
              <br>
              <em>UIST</em>, 2021
              <br>
              <!-- <a href="https://yifanjiang.net/MalleConv.html">project page</a> -->
              <!-- / -->
              <a href="https://thomaskosch.com/wp-content/papercite-data/pdf/bethge2021vemotion.pdf">paper</a> /
              <a href="https://github.com/davebeght/VEmotion">code</a>
              <p></p>
              <p>
              Remote sensing system that analyzes traffic dynamics, environmental factors, in-vehicle context, and road characteristics to implicitly classify driver emotions.
              </p>
            </td>
          </tr>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <!-- <tr onmouseout="malle_stop()" onmouseover="malle_start()"> -->
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='malle_image'>
                    <img src='images/soundsride.png' width="160"></div>
                  <img src='images/soundsride.png' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://3dvar.com/Kari2021SoundsRide.pdf">
                  <papertitle>SoundsRide: Affordance-Synchronized Music Mixing for In-Car Audio Augmented Reality</papertitle>
                </a>
                <br>
                <a href="https://mkari.de/">Mohamed Kari</a>,
                <a href="http://grosse-puppendahl.com/">Tobias Grosse-Puppendahl</a>,
                Alexander Jagaciak,
                <strong>David Bethge</strong>,
                Reinhard Schütte,
                <a href="https://www.christianholz.net/">Christian Holz</a>
                <br>
                <em>UIST</em>, 2021 (Best Paper Award)
                <br>
                <!-- <a href="https://yifanjiang.net/MalleConv.html">project page</a> -->
                <!-- / -->
                <a href="https://3dvar.com/Kari2021SoundsRide.pdf">paper</a>
                <p></p>
                <p>
                  In-car audio augmented reality system that mixes music in real-time synchronized with sound affordances along the ride.
                </p>
              </td>
            </tr>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <!-- <tr onmouseout="malle_stop()" onmouseover="malle_start()"> -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/ismar.png' width="160"></div>
                    <img src='images/ismar.png' width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://mkari.de/pubs/ismar2021-transformr.pdf">
                    <papertitle>TransforMR: Pose-Aware Object Substitution for Composing Alternate Mixed Realities</papertitle>
                  </a>
                  <br>
                  <a href="https://mkari.de/">Mohamed Kari</a>,
                  <a href="http://grosse-puppendahl.com/">Tobias Grosse-Puppendahl</a>,
                  Luis Falconeri Coelho,
                  Andreas Fender,
                  <strong>David Bethge</strong>,
                  Reinhard Schütte,
                  <a href="https://www.christianholz.net/">Christian Holz</a>
                  <br>
                  <em>ISMAR</em>, 2021
                  <br>
                  <!-- <a href="https://yifanjiang.net/MalleConv.html">project page</a> -->
                  <!-- / -->
                  <a href="https://mkari.de/pubs/ismar2021-transformr.pdf">paper</a>
                  <p></p>
                  <p>
                    Mixed reality system for mobile devices that performs 3D-pose-aware object substitution to create meaningful mixed reality scenes.
                  </p>
                </td>
              </tr>



            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <!-- <tr onmouseout="malle_stop()" onmouseover="malle_start()"> -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/hminference.png' width="160"></div>
                    <img src='images/hminference.png' width="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://dl.acm.org/doi/abs/10.1145/3409118.3475145">
                    <papertitle>HMInference: Inferring Multimodal HMI Interactions in Automotive Screens</papertitle>
                  </a>
                  <br>
                  <a>Jannik Wolf</a>,
                  <a>Marco Wiedner</a>
                  <a href="https://mkari.de/">Mohamed Kari</a>,
                  <strong>David Bethge</strong>
                  <br>
                  <em>AutoUI</em>, 2021
                  <br>
                  <!-- <a href="https://yifanjiang.net/MalleConv.html">project page</a> -->
                  <!-- / -->
                  <a href="https://dl.acm.org/doi/abs/10.1145/3409118.3475145">paper</a>
                  <p></p>
                  <p>
                    System to predict interactions in the car HMI based on contextual CAN-BUS data.
                  </p>
                </td>
              </tr>


              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                <!-- <tr onmouseout="malle_stop()" onmouseover="malle_start()"> -->
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='malle_image'>
                        <img src='images/icdm.jpg' width="160"></div>
                      <img src='images/icdm.jpg' width="160">
                    </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <a href="https://ieeexplore.ieee.org/document/8955539">
                      <papertitle>Prognostication of Neurological Recovery by Analyzing Structural Breaks in EEG Data</papertitle>
                    </a>
                    <br>
                    <strong>David Bethge</strong>,
                    <a>Jieshi Chen</a>,
                    <a>Oliver Grothe</a>,
                    <a>Jonathan Elmer</a>,
                    <a>Artur Dubrawski</a>
                    <br>
                    <em>ICDM Workshop</em>, 2019
                    <br>
                    <!-- <a href="https://yifanjiang.net/MalleConv.html">project page</a> -->
                    <!-- / -->
                    <a href="https://ieeexplore.ieee.org/document/8955539">paper</a>
                    <p></p>
                    <p>
                      Unsupervised, multivariate yet interpretable structural break testing for prognostication of neurological recovery after cardiac arrest.
                    </p>
                  </td>
                </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Patents</heading>
                <p>
                  P1: DE/102022120257A1, “Verfahren, Wearable, Vorrichtung und Fahrzeug zur Emotionalisierung eines Fahrererlebnisses im Fahrzeug” (2024)
                </p>
                <p>
                  P2: US/20240027215A1, “Computer-implemented method for determining a navigation route” (2024)
                </p>
                <p>
                  P3: DE/102022113585A1, “Computerimplementiertes Verfahren zum Verbessern einer Performance einer Cockpit-Nutzerschnittstelle“(2023)
                </p>
                <p>
                  P4: DE/102022110349A1, “Verfahren und Vorrichtung zur Überwachung von Objekten” (2023)
                </p>
                <p>
                  P5: DE/102022106797B3, “Verfahren zur automatischen Einstellung zumindest eines Rückspiegels eines Kraftfahrzeugs” (2023)
                </p>
                <p>
                  P6: DE/102022106812B4, “Computerimplementiertes Verfahren zur Ermittlung eines Emotionszustandes einer Person in einem Kraftfahrzeug” (2024)
                </p>
                <p>
                  P7: DE/102022104325A1, “Verfahren, System und Computerprogrammprodukt zur Erstellung einer Datenstruktur für auf künstlicher Intelligenz basierende Projekte” (2023)
                </p>
                <p>
                  P8: DE102022104322A1, “Vorrichtung und Verfahren zur Fahrerassistenz für ein Kraftfahrzeug, die Vorrichtung umfassendes Kraftfahrzeug” (2023)
                </p>
                <p>
                  P9: DE/102021130939B3, “Vorrichtungen und Verfahren zur gemeinsamen Routenführung” (2023)
                </p>
                <p>
                  P10: US/20230147024A1, “Method and system for robust identification of a vehicle occupant” (2023)
                </p>
                <p>
                  P11: DE/102021129108A1, “Verfahren, System und Computerprogrammprodukt zum Ausgeben von Ausgabewerten zur Analyse und/oder Bewertung und/oder Prozesssteuerung einer Entität” (2023)
                </p>
                <p>
                  P12: US/20230146013A1, “Method for producing a model for automated prediction of interactions of a user with a user interface of a motor vehicle” (2023)
                </p>
                <p>
                  P13: DE/102021129094A1, “Verfahren zum Ermitteln von Ersatzteilbedarf” (2023)
                </p>
                <p>
                  P14: US/20230020786A1 “System for a motor vehicle and method for assessing the emotions of a driver of a motor vehicle” (2023)
                </p>
                <p>
                  P15: DE/102021116641A1, “Verfahren, System und Computerprogramm zur interaktiven Auswahl und Wiedergabe von in Echtzeit erzeugten Audio- und/oder Videosequenzen in einem Kraftfahrzeug” (2022)
                </p>
                <p>
                  P16: DE/102021110268A1, “Verfahren und System zur szenensynchronen Auswahl und Wiedergabe von Audiosequenzen für ein Kraftfahrzeug” (2022)
                </p>
              </td>
            </tr>

        





        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Talks / Lectures / Podcasts</heading>
              <p>
                I am regularly giving talks and keynotes about technology in general. Feel free to reach out.              
              </p>             
            </td>
          </tr>
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <tr onmouseout="malle_stop()" onmouseover="malle_start()"> -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                 <!--  <img src='images/doktoranden-podcast-logo.png' width="160"></div>
                <img src='images/doktoranden-podcast-logo.png' width="160"> -->
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle> Visiting Lecturer for a Machine Learning introductory course at Baden-Wuerrtemberg Cooperative State University</papertitle>
              <br>
              <p>
                In 2018 along with a fellow student I lectured a machine learning introductory class for computer science students at Baden-Wuerrtemberg Cooperative State University (DHBW Karlsruhe). 
                <a href="https://github.com/ferreirafabio/Intro_to_ML_DHBW">[lecture material]</a>
              </p>
            </td>
          </tr>
        </tbody>
        </table>




<!--
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <tr onmouseout="malle_stop()" onmouseover="malle_start()"> 
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <img src='images/hilti_view.jpeg' width="160"></div>
                <img src='images/hilti_view.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a>
                <papertitle>Invited talks</papertitle>
              </a>
              <br>
              <p>
                At Hilti, I have initiated, produced, and together with my fellow head of the PhD network Mohamed Kari, co-hosted the Porsche PhD Podcast where we interviewed Porsche PhD students company-internally on their research and Porsche executives on the role of PhD research for innovation.</p>
            </td>
          </tr>
        </tbody>
        </table>

 -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- <tr onmouseout="malle_stop()" onmouseover="malle_start()"> -->
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <img src='images/doktoranden-podcast-logo.png' width="160"></div>
                <img src='images/doktoranden-podcast-logo.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle> Porsche PhD Podcast</papertitle>
              <br>
              <p>
                At Porsche, I have initiated, produced, and together with my fellow head of the PhD network Mohamed Kari, co-hosted the Porsche PhD Podcast where we interviewed Porsche PhD students company-internally on their research and Porsche executives on the role of PhD research for innovation.</p>
            </td>
          </tr>
        </tbody>
        </table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Personal Life</heading>
            <p>
              I come from a family of engineers and entrepeneurs. I grew up in Germany and Austria traveling a lot while I was a kid. I was raised with a strong emphasis on education, lifelong learning, kindness, and the strong desire to think in-depth about the world around us.
            </p>
            <p>  
              In my free time, I find joy climbing in the alpes with my friends and surfing the waves along Europe's coastlines. 
            </p>
          </td>
        </tr>
        </tbody>
      </table>






        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                This website was forked from Jon Barron's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. Thanks!
                <br>
                <!-- Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page. -->
              </p>
            </td>
          </tr>
        </tbody></table>
       </td>
    </tr>
  </table>
  <!--  <div class='impressum'><h1>Impressum</h1><p>Angaben gemäß § 5 TMG</p><p>David Bethge<br> 
    Baiersdorferstr 12<br> 
    22529 Hamburg <br> 
    E-Mail: <a href='mailto:david.bethge@ifi.lmu.de'>david.bethge@ifi.lmu.de</a></br></p><p> <br>
    <p><strong>Haftungsausschluss: </strong><br><br><strong>Haftung für Inhalte</strong><br><br>
    Die Inhalte unserer Seiten wurden mit größter Sorgfalt erstellt. Für die Richtigkeit, Vollständigkeit und Aktualität der Inhalte können wir jedoch keine Gewähr übernehmen. Als Diensteanbieter sind wir gemäß § 7 Abs.1 TMG für eigene Inhalte auf diesen Seiten nach den allgemeinen Gesetzen verantwortlich. Nach §§ 8 bis 10 TMG sind wir als Diensteanbieter jedoch nicht verpflichtet, übermittelte oder gespeicherte fremde Informationen zu überwachen oder nach Umständen zu forschen, die auf eine rechtswidrige Tätigkeit hinweisen. Verpflichtungen zur Entfernung oder Sperrung der Nutzung von Informationen nach den allgemeinen Gesetzen bleiben hiervon unberührt. Eine diesbezügliche Haftung ist jedoch erst ab dem Zeitpunkt der Kenntnis einer konkreten Rechtsverletzung möglich. Bei Bekanntwerden von entsprechenden Rechtsverletzungen werden wir diese Inhalte umgehend entfernen.<br><br><strong>Haftung für Links</strong><br><br>
    Unser Angebot enthält Links zu externen Webseiten Dritter, auf deren Inhalte wir keinen Einfluss haben. Deshalb können wir für diese fremden Inhalte auch keine Gewähr übernehmen. Für die Inhalte der verlinkten Seiten ist stets der jeweilige Anbieter oder Betreiber der Seiten verantwortlich. Die verlinkten Seiten wurden zum Zeitpunkt der Verlinkung auf mögliche Rechtsverstöße überprüft. Rechtswidrige Inhalte waren zum Zeitpunkt der Verlinkung nicht erkennbar. Eine permanente inhaltliche Kontrolle der verlinkten Seiten ist jedoch ohne konkrete Anhaltspunkte einer Rechtsverletzung nicht zumutbar. Bei Bekanntwerden von Rechtsverletzungen werden wir derartige Links umgehend entfernen.<br><br><strong>Urheberrecht</strong><br><br>
    Die durch die Seitenbetreiber erstellten Inhalte und Werke auf diesen Seiten unterliegen dem deutschen Urheberrecht. Die Vervielfältigung, Bearbeitung, Verbreitung und jede Art der Verwertung außerhalb der Grenzen des Urheberrechtes bedürfen der schriftlichen Zustimmung des jeweiligen Autors bzw. Erstellers. Downloads und Kopien dieser Seite sind nur für den privaten, nicht kommerziellen Gebrauch gestattet. Soweit die Inhalte auf dieser Seite nicht vom Betreiber erstellt wurden, werden die Urheberrechte Dritter beachtet. Insbesondere werden Inhalte Dritter als solche gekennzeichnet. Sollten Sie trotzdem auf eine Urheberrechtsverletzung aufmerksam werden, bitten wir um einen entsprechenden Hinweis. Bei Bekanntwerden von Rechtsverletzungen werden wir derartige Inhalte umgehend entfernen.<br><br><strong>Datenschutz</strong><br><br>
    Die Nutzung unserer Webseite ist in der Regel ohne Angabe personenbezogener Daten möglich. Soweit auf unseren Seiten personenbezogene Daten (beispielsweise Name, Anschrift oder eMail-Adressen) erhoben werden, erfolgt dies, soweit möglich, stets auf freiwilliger Basis. Diese Daten werden ohne Ihre ausdrückliche Zustimmung nicht an Dritte weitergegeben. <br>
    Wir weisen darauf hin, dass die Datenübertragung im Internet (z.B. bei der Kommunikation per E-Mail) Sicherheitslücken aufweisen kann. Ein lückenloser Schutz der Daten vor dem Zugriff durch Dritte ist nicht möglich. <br>
    Der Nutzung von im Rahmen der Impressumspflicht veröffentlichten Kontaktdaten durch Dritte zur Übersendung von nicht ausdrücklich angeforderter Werbung und Informationsmaterialien wird hiermit ausdrücklich widersprochen. Die Betreiber der Seiten behalten sich ausdrücklich rechtliche Schritte im Falle der unverlangten Zusendung von Werbeinformationen, etwa durch Spam-Mails, vor.<br>
    <br><br><strong>Google Analytics</strong><br><br>
    Diese Website benutzt Google Analytics, einen Webanalysedienst der Google Inc. (''Google''). Google Analytics verwendet sog. ''Cookies'', Textdateien, die auf Ihrem Computer gespeichert werden und die eine Analyse der Benutzung der Website durch Sie ermöglicht. Die durch den Cookie erzeugten Informationen über Ihre Benutzung dieser Website (einschließlich Ihrer IP-Adresse) wird an einen Server von Google in den USA übertragen und dort gespeichert. Google wird diese Informationen benutzen, um Ihre Nutzung der Website auszuwerten, um Reports über die Websiteaktivitäten für die Websitebetreiber zusammenzustellen und um weitere mit der Websitenutzung und der Internetnutzung verbundene Dienstleistungen zu erbringen. Auch wird Google diese Informationen gegebenenfalls an Dritte übertragen, sofern dies gesetzlich vorgeschrieben oder soweit Dritte diese Daten im Auftrag von Google verarbeiten. Google wird in keinem Fall Ihre IP-Adresse mit anderen Daten der Google in Verbindung bringen. Sie können die Installation der Cookies durch eine entsprechende Einstellung Ihrer Browser Software verhindern; wir weisen Sie jedoch darauf hin, dass Sie in diesem Fall gegebenenfalls nicht sämtliche Funktionen dieser Website voll umfänglich nutzen können. Durch die Nutzung dieser Website erklären Sie sich mit der Bearbeitung der über Sie erhobenen Daten durch Google in der zuvor beschriebenen Art und Weise und zu dem zuvor benannten Zweck einverstanden.<br><br><strong>Google AdSense</strong><br><br>
    Diese Website benutzt Google Adsense, einen Webanzeigendienst der Google Inc., USA (''Google''). Google Adsense verwendet sog. ''Cookies'' (Textdateien), die auf Ihrem Computer gespeichert werden und die eine Analyse der Benutzung der Website durch Sie ermöglicht. Google Adsense verwendet auch sog. ''Web Beacons'' (kleine unsichtbare Grafiken) zur Sammlung von Informationen. Durch die Verwendung des Web Beacons können einfache Aktionen wie der Besucherverkehr auf der Webseite aufgezeichnet und gesammelt werden. Die durch den Cookie und/oder Web Beacon erzeugten Informationen über Ihre Benutzung dieser Website (einschließlich Ihrer IP-Adresse) werden an einen Server von Google in den USA übertragen und dort gespeichert. Google wird diese Informationen benutzen, um Ihre Nutzung der Website im Hinblick auf die Anzeigen auszuwerten, um Reports über die Websiteaktivitäten und Anzeigen für die Websitebetreiber zusammenzustellen und um weitere mit der Websitenutzung und der Internetnutzung verbundene Dienstleistungen zu erbringen. Auch wird Google diese Informationen gegebenenfalls an Dritte übertragen, sofern dies gesetzlich vorgeschrieben oder soweit Dritte diese Daten im Auftrag von Google verarbeiten. Google wird in keinem Fall Ihre IP-Adresse mit anderen Daten der Google in Verbindung bringen. Das Speichern von Cookies auf Ihrer Festplatte und die Anzeige von Web Beacons können Sie verhindern, indem Sie in Ihren Browser-Einstellungen ''keine Cookies akzeptieren'' wählen (Im MS Internet-Explorer unter ''Extras > Internetoptionen > Datenschutz > Einstellung''; im Firefox unter ''Extras > Einstellungen > Datenschutz > Cookies''); wir weisen Sie jedoch darauf hin, dass Sie in diesem Fall gegebenenfalls nicht sämtliche Funktionen dieser Website voll umfänglich nutzen können. Durch die Nutzung dieser Website erklären Sie sich mit der Bearbeitung der über Sie erhobenen Daten durch Google in der zuvor beschriebenen Art und Weise und zu dem zuvor benannten Zweck einverstanden.</p><br> 
    Website Impressum erstellt durch <a href="https://www.impressum-generator.de">impressum-generator.de</a> von der <a href="https://www.kanzlei-hasselbach.de/" rel="nofollow">Kanzlei Hasselbach</a>
     </div>
     -->

     
</body>

</html>
